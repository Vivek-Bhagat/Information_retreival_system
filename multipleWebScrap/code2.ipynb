{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Data has been written to nature_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_page(page_number):\n",
    "    url = f'https://www.nature.com/nature/articles?searchType=journalSearch&sort=PubDate&page={page_number}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_number}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    for article in soup.find_all('article', class_='c-card'):\n",
    "        title_main = article.find('span', class_='c-meta__type')\n",
    "        title_m = title_main.get_text(strip=True) if title_main else 'N/A'\n",
    "\n",
    "\n",
    "        title_tag = article.find('h3', class_='c-card__title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "        \n",
    "        summary_tag = article.find('div', class_='c-card__summary')\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else 'N/A'\n",
    "        \n",
    "        author_tag = article.find('ul', class_='c-author-list')\n",
    "        authors = [author.get_text(strip=True) for author in author_tag.find_all('span', itemprop='name')] if author_tag else 'N/A'\n",
    "        author = ', '.join(authors)\n",
    "        \n",
    "        date_tag = article.find('time', itemprop='datePublished')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else 'N/A'\n",
    "        \n",
    "        \n",
    "\n",
    "        articles.append([title_m,title, summary, author, date])\n",
    "\n",
    "    return articles\n",
    "\n",
    "def main(start_page, end_page):\n",
    "    all_articles = []\n",
    "\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping page {page_number}...\")\n",
    "        articles = scrape_page(page_number)\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "    \n",
    "    with open('nature_articless.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Type','Title', 'Summary', 'Author', 'Publication Date'])\n",
    "        writer.writerows(all_articles)\n",
    "\n",
    "    print(\"Data has been written to nature_articles.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    start_page = 1\n",
    "    end_page = 50 \n",
    "    main(start_page, end_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('nature_articless.csv')\n",
    "\n",
    "# Normalize the text\n",
    "def normalize_text(text):\n",
    "    text = contractions.fix(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['normalized_text'] = df['Title'].apply(normalize_text)\n",
    "\n",
    "# Tokenization\n",
    "df['tokens'] = df['normalized_text'].apply(word_tokenize)\n",
    "\n",
    "# Stop-Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Stemming or Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['stemmed_words'] = df['filtered_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "df['lemmatized_words'] = df['filtered_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "df.to_csv('processed_datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ontology = {\n",
    "    \"Article Types\": {\n",
    "        \"Research Paper\": {\n",
    "            \"Topics\": [\"Machine Learning\", \"Climate Change\", \"Biology\"]\n",
    "        },\n",
    "        \"Review Article\": {\n",
    "            \"Topics\": [\"Deep Learning\", \"Genomics\", \"Ecology\"]\n",
    "        },\n",
    "        \"Commentary\": {\n",
    "            \"Topics\": [\"Policy\", \"Ethics\", \"Society\"]\n",
    "        }\n",
    "    },\n",
    "    \"Authors\": {\n",
    "        \"Research Paper\": [\"Author A\", \"Author B\"],\n",
    "        \"Review Article\": [\"Author C\", \"Author D\"],\n",
    "        \"Commentary\": [\"Author E\", \"Author F\"]\n",
    "    },\n",
    "    \"Publication Dates\": {\n",
    "        \"Research Paper\": \"2024-08-01\",\n",
    "        \"Review Article\": \"2024-07-15\",\n",
    "        \"Commentary\": \"2024-07-20\"\n",
    "    },\n",
    "    \"Synonyms\": {\n",
    "        \"AI\": \"Artificial Intelligence\",\n",
    "        \"ML\": \"Machine Learning\"\n",
    "    },\n",
    "    \"Antonyms\": {\n",
    "        \"Hot\": \"Cold\",\n",
    "        \"Increase\": \"Decrease\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example usage: Check if a topic exists in a particular article type\n",
    "def check_topic(article_type, topic):\n",
    "    return topic in ontology['Article Types'].get(article_type, {}).get(\"Topics\", [])\n",
    "\n",
    "print(check_topic(\"Research Paper\", \"Machine Learning\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
